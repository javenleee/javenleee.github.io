<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">











<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">


















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.7.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: true,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


<!--站长验证-->
<meta name="baidu-site-verification" content="5rEIjow3aW">
<meta name="google-site-verification" content="OhdtVOx5uwpZ_mMm0AZJXzw-dY1PPpAAkdavmmQhIL4">
<meta name="360-site-verification" content="5b1c9d7574859ca6e460dd687667d5dc">



  <meta name="description" content="NLP课程大作业，聊天机器人">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理项目">
<meta property="og:url" content="https://www.shmily.today/聊天机器人.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="NLP课程大作业，聊天机器人">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105142690.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105153674.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105214745.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105726146.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104110208445.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104110237978.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104110247009.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104111442432.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104112443394.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104113139529.png">
<meta property="og:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104120135014.png">
<meta property="og:updated_time" content="2022-09-07T11:18:15.781Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="自然语言处理项目">
<meta name="twitter:description" content="NLP课程大作业，聊天机器人">
<meta name="twitter:image" content="https://www.shmily.today/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105142690.png">



  <link rel="alternate" href="/atom.xml" title="blog" type="application/atom+xml">




  <link rel="canonical" href="https://www.shmily.today/聊天机器人.html">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>自然语言处理项目 | blog</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博采众长</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">心有猛虎，细嗅蔷薇</h1>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>




<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-guestbook">

    
    
    
      
    

    

    <a href="/guestbook/" rel="section"><i class="menu-item-icon fa fa-fw fa-book"></i> <br>留言</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
		
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result">
    
      <div style="text-align: center;padding: 3px 0 0;">
       <div style="margin-top: 20px;font-size: 18px;font-weight: 600;border-bottom: 1px solid #ccc;">
         <i class="fa fa-history" aria-hidden="true"></i>
         近期文章
       </div>
       <ul style="margin: 0;padding: 0;list-style: none;">
         
         
           <li>
             <a href="/区块链确定性指南-译.html" title="区块链确定性指南(译)" target="_blank">区块链确定性指南(译)</a>
           </li>
         
           <li>
             <a href="/区块链相关期刊会议.html" title="区块链相关期刊会议" target="_blank">区块链相关期刊会议</a>
           </li>
         
           <li>
             <a href="/区块链方向相关顶会.html" title target="_blank"></a>
           </li>
         
           <li>
             <a href="/Fabric之Raft共识深入介绍_乌苏舞的博客-CSDN博客_fabric raft.html" title="Fabric之Raft共识深入介绍" target="_blank">Fabric之Raft共识深入介绍</a>
           </li>
         
           <li>
             <a href="/周报.html" title target="_blank"></a>
           </li>
         
           <li>
             <a href="/hyperledger-fabric-源码阅读.html" title="hyperledger fabric 源码阅读" target="_blank">hyperledger fabric 源码阅读</a>
           </li>
         
           <li>
             <a href="/买面粉.html" title="买面粉" target="_blank">买面粉</a>
           </li>
         
           <li>
             <a href="/leetcode-1637-两点之间不包含任何点的最宽垂直面积.html" title="leetcode-1637-两点之间不包含任何点的最宽垂直面积" target="_blank">leetcode-1637-两点之间不包含任何点的最宽垂直面积</a>
           </li>
         
           <li>
             <a href="/leetcode.html" title target="_blank"></a>
           </li>
         
           <li>
             <a href="/leetcode-1109-航班预定统计.html" title="leetcode-1109-航班预定统计" target="_blank">leetcode-1109-航班预定统计</a>
           </li>
         
           <li>
             <a href="/leetcode-1524-和为奇数的子数组数目.html" title="leetcode-1524-和为奇数的子数组数目" target="_blank">leetcode-1524-和为奇数的子数组数目</a>
           </li>
         
           <li>
             <a href="/BlockChain.html" title target="_blank"></a>
           </li>
         
           <li>
             <a href="/zksnark.html" title target="_blank"></a>
           </li>
         
           <li>
             <a href="/leetcode-11-盛最多水的容器.html" title="leetcode-11.盛最多水的容器" target="_blank">leetcode-11.盛最多水的容器</a>
           </li>
         
           <li>
             <a href="/leetcode-474-一和零.html" title="leetcode-474.一和零" target="_blank">leetcode-474.一和零</a>
           </li>
         
       </ul>
      </div>
    
  </div>
</div>



    </div>
  

</nav>



  



</div>
    </header>

    
  
  
  
    
      
    
    <a href="https://github.com/Zosien/zosien.github.io" class="github-corner" target="_blank" title="万水千山总是情,给个star行不行！" aria-label="万水千山总是情,给个star行不行！"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#222; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg>
    
      </a>
    



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  
    <div class="reading-progress-bar"></div>
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.shmily.today/聊天机器人.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Javen">
      <meta itemprop="description" content="柔而不弱，强而不悍<br/>阳光下像个孩子<br/>风雨里像个大人">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                自然语言处理项目
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2021-11-20 21:09:04" itemprop="dateCreated datePublished" datetime="2021-11-20T21:09:04+08:00">2021-11-20</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                
                <span class="post-meta-item-text"> 评论数： </span>
                
                <a href="/聊天机器人.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/聊天机器人.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/聊天机器人.html" class="leancloud_visitors" data-flag-title="自然语言处理项目">
               <span class="post-meta-divider">|</span>
               <span title="阅读次数">
                 <span class="post-meta-item-icon">
                   <i class="fa fa-eye"></i>
                 </span>
                 
                   <span class="post-meta-item-text">阅读次数：</span>
                 
                   <span class="leancloud-visitors-count"></span>
               </span>
             </span>
          

          
          
          

          

          
              <div class="post-description">NLP课程大作业，聊天机器人</div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="聊天机器人技术报告"><a href="#聊天机器人技术报告" class="headerlink" title="聊天机器人技术报告"></a>聊天机器人技术报告</h2><p>本项目复现现有开源项目，基于seq2seq+attention实现聊天机器人。</p>
<p>seq2seq模型简介：</p>
<p>经典的RNN模型，输入和输出个数有一定限制，但在聊天场景中，输入和输出个数肯定是不固定的，因此不能采用经典RNN，而采用seq2seq，Seq2Seq 是一种重要的 RNN 模型，也称为 Encoder-Decoder 模型，可以理解为一种 <strong>N×M</strong> 的模型。模型包含两个部分：<strong>Encoder</strong> 用于编码序列的信息，将任意长度的序列信息编码到一个向量 <strong>c</strong> 里。而 <strong>Decoder</strong> 是解码器，解码器得到上下文信息向量 <strong>c</strong> 之后可以将信息解码，并输出为序列。Seq2Seq 模型结构有很多种，下面是几种比较常见的：</p>
<p><strong>第一种</strong></p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105142690.png" alt="image-20211104105142690"></p>
<p><strong>第二种</strong></p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105153674.png" alt="image-20211104105153674"></p>
<p><strong>第三种</strong></p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105214745.png" alt="image-20211104105214745"></p>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>这三种Seq2Seq模型的主要区别在于Decoder，他们的Encoder是一样的，下图是Encoder部分，Encoder的RNN接受输入x，最终输出一个编码所有信息的上下文变量c，中间的神经元没有输出。Decoder主要传入的是上下文向量c，然后解码出重要信息。</p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104105726146.png" alt="image-20211104105726146"></p>
<p>Encoder与RNN区别不大，只是中间神经元没有输出。上下文变量c可以采用多种方式计算。如</p>
<script type="math/tex; mode=display">
C = h_N \\
c = q(h_N) \\
c = q(h_1,h_2,...,h_N)</script><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>Decoder有多种不同的结构，这里主要介绍三种。</p>
<p><strong>第一种</strong></p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104110208445.png" alt="image-20211104110208445"></p>
<p>上下文变量c当成是RNN的初始隐藏状态，输入到RNN中，后续只接受上一个神经元的隐藏层状态$h’$而不接收其他的输入x</p>
<p><strong>第二种</strong></p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104110237978.png" alt="image-20211104110237978"></p>
<p>第二种Decoder有了自己的初始隐藏层状态$h’_0$ 不再把上下文向量c当成是RNN的初始隐藏层状态，而是当成RNN每一个神经元的输入。每个神经元都拥有相同的输入c</p>
<p><strong>第三种</strong></p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104110247009.png" alt="image-20211104110247009"></p>
<p>第三种Decoder与第二种类似，但是在输入部分多了上一个神经元的输出y‘，即每一个神经元的输入包括：上一个神经元的隐藏层向量h’，上一个神经元的输出y‘，当前的输入c。对于第一个神经元的输入$y’_0$ 通常是句子起始标志位的embedding向量。</p>
<h3 id="Teacher-Forcing"><a href="#Teacher-Forcing" class="headerlink" title="Teacher Forcing"></a>Teacher Forcing</h3><p>针对第三种模型，他的当前输出会传递给下一个神经元作为输入，如果当前神经元的输出是错误的，那么下一个神经元的输出也很容易出错，导致错误会一直传递下去。</p>
<p>而Teacher Forcing可以在一定程度上缓解上面的问题，在训练Seq2Seq模型时，Decoder的每一个神经元并非一定使用上一个神经元的输出，而是有一定的比例采用正确的序列作为输入。</p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104111442432.png" alt="image-20211104111442432"></p>
<h3 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h3><p>在Seq2Seq模型，Encoder总是将源句子的所有信息编码到一个固定长度的上下文变量c中，然后在Decoder编码过程中向量c都是不变的。这存在不少缺陷：</p>
<ul>
<li>对于比较长的句子，很难用一个定长的向量c完全表示其意义。</li>
<li>RNN存在长序列梯度消失的问题，只使用最后一个神经元得到的向量c效果不理想</li>
<li>与人类的注意力方式不同，即人类在阅读文章的时候，会把注意力放在当前的句子上</li>
</ul>
<p>Attention即注意力机制，是一种将模型的注意力放在当前翻译单词上的一种机制，例如翻译“I have a cat”时，翻译到“我”时，要将注意力放在原句的“I”上，翻译到“猫”时，要将注意力放在原句的“cat”上。使用Attention后，Decoder的输入就不是固定的上下文变量c了，而是会根据当前翻译的信息计算当前的c。</p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104112443394.png" alt="image-20211104112443394"></p>
<p>本文使用Bahdanau attention机制，Bahdanau本质是一种 <strong>加性attention机制</strong>，将decoder的隐状态和encoder所有位置输出通过线性组合对齐，得到context向量，用于改善序列到序列的翻译模型。</p>
<p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104113139529.png" alt="image-20211104113139529"></p>
<h3 id="关键代码分析："><a href="#关键代码分析：" class="headerlink" title="关键代码分析："></a>关键代码分析：</h3><p><strong>Encoder</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个Encoder类，实现Encoder-Decoder结构中的Encoder部分</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">  <span class="comment"># 定义初始化函数，将形参初始化</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, enc_units, batch_sz)</span>:</span></span><br><span class="line">    super(Encoder, self).__init__()</span><br><span class="line">    self.batch_sz = batch_sz</span><br><span class="line">    self.enc_units = enc_units</span><br><span class="line">    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)</span><br><span class="line">    self.gru = tf.keras.layers.GRU(self.enc_units,return_sequences=<span class="literal">True</span>,return_state=<span class="literal">True</span>,</span><br><span class="line">                                   recurrent_initializer=<span class="string">'glorot_uniform'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 定义执行函数，所有的算法逻辑执行都在call函数中完成</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, hidden)</span>:</span></span><br><span class="line">    <span class="comment"># 对输入的序列进行embedding</span></span><br><span class="line">    x = self.embedding(x)</span><br><span class="line">    <span class="comment"># 将embedding的结果输入gru神经网络层，得到输出结果和神经元状态</span></span><br><span class="line">    <span class="comment"># gru和lstm类似，能够解决长序列下的梯度消失和梯度爆炸问题</span></span><br><span class="line">    output, state = self.gru(x, initial_state = hidden)</span><br><span class="line">    <span class="comment"># 返回输出结果和神经元状态</span></span><br><span class="line">    <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 进入隐藏层初始化函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize_hidden_state</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 使用全零矩阵进行初始化</span></span><br><span class="line">    <span class="keyword">return</span> tf.zeros((self.batch_sz, self.enc_units))</span><br></pre></td></tr></table></figure>
<p><strong>Attention</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauAttention</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">  <span class="comment"># 定义初始化函数，将形参初始化</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units)</span>:</span></span><br><span class="line">    super(BahdanauAttention, self).__init__()</span><br><span class="line">    <span class="comment"># 分别使用神经网络全连接层初始化W1 W2 V，作为计算Q K V的算法</span></span><br><span class="line">    self.W1 = tf.keras.layers.Dense(units)</span><br><span class="line">    self.W2 = tf.keras.layers.Dense(units)</span><br><span class="line">    self.V = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 定义执行函数，所有的算法逻辑执行都在call函数中完成</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, query, values)</span>:</span></span><br><span class="line">    <span class="comment"># 将query序列增加一个维度</span></span><br><span class="line">    hidden_with_time_axis = tf.expand_dims(query, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 使用W1 W2 V计算attention值，也就是score。在计算的过程中，将W1和W2d的计算结果进行了一次非线性变换</span></span><br><span class="line">    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))</span><br><span class="line">    <span class="comment"># 使用softmax将score中的元素值按行转换成概率分布作为attention的权重值</span></span><br><span class="line">    attention_weights = tf.nn.softmax(score, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 将获得attention权重与输入的序列相乘得到语境向量</span></span><br><span class="line">    context_vector = attention_weights * values</span><br><span class="line">    <span class="comment"># 将语境向量按行求和，得到最后的语境向量</span></span><br><span class="line">    context_vector = tf.reduce_sum(context_vector, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 最后返回语境向量和attention权重值</span></span><br><span class="line">    <span class="keyword">return</span> context_vector, attention_weights</span><br></pre></td></tr></table></figure>
<p><strong>Decoder</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个Decoder类，实现Encoder-Decoder结构中的Decoder部分</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, dec_units, batch_sz)</span>:</span></span><br><span class="line">    <span class="comment"># 定义初始化函数，将形参初始化</span></span><br><span class="line">    super(Decoder, self).__init__()</span><br><span class="line">    <span class="comment"># 初始化batch_size</span></span><br><span class="line">    self.batch_sz = batch_sz</span><br><span class="line">    <span class="comment"># 初始化神经元数量</span></span><br><span class="line">    self.dec_units = dec_units</span><br><span class="line">    <span class="comment"># 初始化embedding层</span></span><br><span class="line">    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)</span><br><span class="line">    <span class="comment"># 初始化gru</span></span><br><span class="line">    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>,</span><br><span class="line">                                   recurrent_initializer=<span class="string">'glorot_uniform'</span>)</span><br><span class="line">    <span class="comment"># 初始化全连接层</span></span><br><span class="line">    self.fc = tf.keras.layers.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化一个BahdanauAttention</span></span><br><span class="line">    self.attention = BahdanauAttention(self.dec_units)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 定义执行函数，所有的算法逻辑执行都在call函数中完成</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, hidden, enc_output)</span>:</span></span><br><span class="line">    <span class="comment"># 首先使用BahdanauAttention对encode的输出和隐藏状态进行attention计算，输出语境向量和attention权重</span></span><br><span class="line">    context_vector, attention_weights = self.attention(hidden, enc_output)</span><br><span class="line">    <span class="comment"># 对decode的输入序列进行embedding计算</span></span><br><span class="line">    x = self.embedding(x)</span><br><span class="line">    <span class="comment"># 将语境向量增加一个维度后与embedding的结果拼接在一起</span></span><br><span class="line">    x = tf.concat([tf.expand_dims(context_vector, <span class="number">1</span>), x], axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># 将拼接后的结构输入到gru神经网络层，然后返回输出结果和神经元状态</span></span><br><span class="line">    output, state = self.gru(x)</span><br><span class="line">    <span class="comment"># 将输出结果进行维度变换</span></span><br><span class="line">    output = tf.reshape(output, (<span class="number">-1</span>, output.shape[<span class="number">2</span>]))</span><br><span class="line">    <span class="comment"># 将维度变换后的结果输入到输出层，也就是一个全连接神经网络层</span></span><br><span class="line">    output_x = self.fc(output)</span><br><span class="line">    <span class="comment"># 最后返回输出结果，神经元状态和attention的权重</span></span><br><span class="line">    <span class="keyword">return</span> output_x, state, attention_weights</span><br></pre></td></tr></table></figure>
<p><strong>训练函数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个训练函数，以完成对训练集数据的循环训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(inp, targ, targ_lang,enc_hidden)</span>:</span></span><br><span class="line">  <span class="comment"># 初始化loss</span></span><br><span class="line">  loss = <span class="number">0</span></span><br><span class="line">  <span class="comment"># 当我们使用tf.keras.model进行构造模型时，一般采用tf.GradientTape进行半手工计算梯度，然后将梯度给</span></span><br><span class="line">  <span class="comment"># 优化器进行参数优化。with xxx as : 代表以下的操作都是在同一个spacename下进行的</span></span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    <span class="comment"># 首先将训练集输入输出序列和enc_hidden的初始化作为encoder的输入</span></span><br><span class="line">    enc_output, enc_hidden = encoder(inp, enc_hidden)</span><br><span class="line">    <span class="comment"># 然后dec_hidden共享enc_hidden的值</span></span><br><span class="line">    dec_hidden = enc_hidden</span><br><span class="line">    <span class="comment"># 使用字典中的start索引构建一个decoder的输入，也就是意味着第一个词的开始</span></span><br><span class="line">    dec_input = tf.expand_dims([targ_lang.word_index[<span class="string">'start'</span>]] * BATCH_SIZE, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 然后接着开始进行强制循环，把输出的词作为下一个循环的decoder的输入</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, targ.shape[<span class="number">1</span>]):</span><br><span class="line">      <span class="comment"># 把输出的词作为下一个循环的decoder的输入</span></span><br><span class="line">      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)</span><br><span class="line">      <span class="comment"># 调用loss function计算训练的loss</span></span><br><span class="line">      loss += loss_function(targ[:, t], predictions)</span><br><span class="line">      <span class="comment"># 使用标注数据来构建decoder的输入</span></span><br><span class="line">      dec_input = tf.expand_dims(targ[:, t], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 计算批量训练的平均loss</span></span><br><span class="line">  batch_loss = (loss / int(targ.shape[<span class="number">1</span>]))</span><br><span class="line">  <span class="comment"># 构造encoder和decoder中可以被优化的参数</span></span><br><span class="line">  variables = encoder.trainable_variables + decoder.trainable_variables</span><br><span class="line">  <span class="comment"># 计算梯度</span></span><br><span class="line">  gradients = tape.gradient(loss, variables)</span><br><span class="line">  <span class="comment"># 使用梯度优化可以被优化的参数</span></span><br><span class="line">  optimizer.apply_gradients(zip(gradients, variables))</span><br><span class="line">  <span class="comment"># 返回批量loss</span></span><br><span class="line">  <span class="keyword">return</span> batch_loss</span><br></pre></td></tr></table></figure>
<p><strong>数据处理过程</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个训练处理函数，就是在训练的前后分别加上start和end</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_sentence</span><span class="params">(w)</span>:</span></span><br><span class="line">    w = <span class="string">'start '</span> + w + <span class="string">' end'</span></span><br><span class="line">    <span class="comment"># print(w)</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取训练集的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(path, num_examples)</span>:</span></span><br><span class="line">    <span class="comment"># 打开数据集文件，按行读取，并去除换行符</span></span><br><span class="line">    lines = io.open(path, encoding=<span class="string">'UTF-8'</span>).read().strip().split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="comment"># 循环为每行读取的数据加上start和end</span></span><br><span class="line">    word_pairs = [[preprocess_sentence(w) <span class="keyword">for</span> w <span class="keyword">in</span> l.split(<span class="string">'\t'</span>)] <span class="keyword">for</span> l <span class="keyword">in</span> lines[:num_examples]]</span><br><span class="line">    <span class="keyword">return</span> zip(*word_pairs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个最大长度求取函数，用于求取最长语句的长度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_length</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> max(len(t) <span class="keyword">for</span> t <span class="keyword">in</span> tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义read_data函数，读取训练集的数据，并对数据进行tokenize处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(path, num_examples)</span>:</span></span><br><span class="line">    <span class="comment"># 将数据进行拆分，也就是拆分成输入集和输出集</span></span><br><span class="line">    input_lang, target_lang = create_dataset(path, num_examples)</span><br><span class="line">    <span class="comment"># 分别对输入数据集和输出数据集进行字符转数字的处理，返回处理后的数字向量和词典</span></span><br><span class="line">    input_tensor, input_token = tokenize(input_lang)</span><br><span class="line">    target_tensor, target_token = tokenize(target_lang)</span><br><span class="line">    <span class="comment"># 最后返回处理后的数字向量和词典</span></span><br><span class="line">    <span class="keyword">return</span> input_tensor, input_token, target_tensor, target_token</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义字符转换函数，其作用是将字符转换为在字典中对应的索引数字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(lang)</span>:</span></span><br><span class="line">    <span class="comment"># 实例化一个tokenizer</span></span><br><span class="line">    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=gConfig[<span class="string">'enc_vocab_size'</span>], oov_token=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 使用fit_on_texts的方法在目标数据集上训练，其实就是构建一个数据集的词典</span></span><br><span class="line">    lang_tokenizer.fit_on_texts(lang)</span><br><span class="line">    <span class="comment"># 使用texts_to_sequences对目标数据集进行字符到数字的转换</span></span><br><span class="line">    tensor = lang_tokenizer.texts_to_sequences(lang)</span><br><span class="line">    <span class="comment"># 最后对转换后的数字向量进行padding</span></span><br><span class="line">    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=max_length_inp, padding=<span class="string">'post'</span>)</span><br><span class="line">    <span class="comment"># 返回数字向量和词典对象</span></span><br><span class="line">    <span class="keyword">return</span> tensor, lang_tokenizer</span><br></pre></td></tr></table></figure>
<p><strong>训练</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练函数，对训练集数据进行循环训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"Preparing data in %s"</span> % gConfig[<span class="string">'train_data'</span>])</span><br><span class="line">    <span class="comment"># 计算将全部训练集数据训练完一遍所需的步数</span></span><br><span class="line">    steps_per_epoch = len(input_tensor) // gConfig[<span class="string">'batch_size'</span>]</span><br><span class="line">    print(steps_per_epoch) <span class="comment">#390</span></span><br><span class="line">    <span class="comment"># 对encoder的隐藏层状态进行初始化</span></span><br><span class="line">    enc_hidden = seq2seqModel.encoder.initialize_hidden_state()</span><br><span class="line">    <span class="comment"># 读取模型保存在文件夹中的数据</span></span><br><span class="line">    checkpoint_dir = gConfig[<span class="string">'model_data'</span>]</span><br><span class="line">    ckpt = tf.io.gfile.listdir(checkpoint_dir)</span><br><span class="line">    checkpoint_prefix = os.path.join(checkpoint_dir, <span class="string">"ckpt"</span>)</span><br><span class="line">    <span class="comment"># 判断是否存在已经训练好的数据模型，如果存在就加载已有的模型并继续进行训练</span></span><br><span class="line">    <span class="keyword">if</span> ckpt:</span><br><span class="line">        print(<span class="string">"reload pretrained model"</span>)</span><br><span class="line">        seq2seqModel.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))</span><br><span class="line">    <span class="comment"># 赋值需要进行随机打乱的数据数量，我们这里使用全局全打乱</span></span><br><span class="line">    BUFFER_SIZE = len(input_tensor)</span><br><span class="line">    <span class="comment"># 使用tf.data.Dataset对训练数据进行一系列的处理，包括数据的随机打乱</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)</span><br><span class="line">    <span class="comment"># 使用batch方法，将数据按照batch_size的大小进行切割，将余数丢掉</span></span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE, drop_remainder=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 获取训练开始时间</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    current_steps = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 开始进行循环训练，设置100次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">        <span class="comment"># 获取每个epoch开始的时间</span></span><br><span class="line">        start_time_epoch = time.time()</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 使用enumerate方法穷举遍历所有的批数据，并调用seq2seqModel中的train_step函数进行训练</span></span><br><span class="line">        <span class="keyword">for</span> (batch, (inp, targ)) <span class="keyword">in</span> enumerate(dataset.take(steps_per_epoch)):</span><br><span class="line">            batch_loss = seq2seqModel.train_step(inp, targ, target_token, enc_hidden)</span><br><span class="line">            total_loss += batch_loss</span><br><span class="line">            print(batch_loss.numpy())</span><br><span class="line">        <span class="comment"># 计算每一步消耗的时间</span></span><br><span class="line">        step_time_epoch = (time.time() - start_time_epoch) / steps_per_epoch</span><br><span class="line">        <span class="comment"># 计算每一步的平均loss</span></span><br><span class="line">        step_loss = total_loss / steps_per_epoch</span><br><span class="line">        <span class="comment"># 计算当前训练的步数</span></span><br><span class="line">        current_steps += steps_per_epoch</span><br><span class="line">        <span class="comment"># 计算全部训练每步的平均时间</span></span><br><span class="line">        step_time_total = (time.time() - start_time) / current_steps</span><br><span class="line">        <span class="comment"># 打印出相关的参数</span></span><br><span class="line">        print(<span class="string">'训练总步数: &#123;&#125; 每步耗时: &#123;&#125;  最新每步耗时: &#123;&#125; 最新每步loss &#123;:.4f&#125;'</span>.format(</span><br><span class="line">            current_steps, step_time_total, step_time_epoch, step_loss.numpy()))</span><br><span class="line">        <span class="comment"># 保存训练完成的模型</span></span><br><span class="line">        seq2seqModel.checkpoint.save(file_prefix=checkpoint_prefix)</span><br><span class="line">        <span class="comment"># 刷新输出屏幕</span></span><br><span class="line">        sys.stdout.flush()</span><br></pre></td></tr></table></figure>
<p><strong>45w+条数据</strong>，即45w+条对话，<strong>设置epoch为100</strong>，<strong>batch_size=128</strong>，多次训练，一共训练了130轮。最后结果<strong>loss在1.4左右</strong>。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/image-20211104120135014.png" alt="image-20211104120135014"></p>
<p>使用源项目作者提供的交互网页测试项目，能进行简单的沟通，多半能正常交流，但有些结果也会答非所问。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://www.jianshu.com/p/80436483b13b" target="_blank" rel="noopener">Seq2Seq 模型详解 - 简书 (jianshu.com)</a></p>
<p><a href="https://blog.csdn.net/daniellibin/article/details/103290169" target="_blank" rel="noopener">基于seq2seq的中文聊天机器人（一）_daniellibin的博客-CSDN博客</a></p>

      
    </div>

    
      


    

    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>感谢支持 ！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById(&quot;QR&quot;); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Javen 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Javen 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Javen</li>
  <li>
  <strong>修改时间： </strong>2022-09-07 19:18:15</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.shmily.today/聊天机器人.html" title="自然语言处理项目">https://www.shmily.today/聊天机器人.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    
      <div class="post-tags">
        
          <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
        
      </div>
    
    
    

    <footer class="post-footer">
      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/蓝桥杯.html" rel="next" title="蓝桥杯整理">
                <i class="fa fa-chevron-left"></i> 蓝桥杯整理
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/zk.html" rel="prev" title>
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          
     
  

  
    <div class="comments" id="comments">
    </div>
    
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Javen">
            
              <p class="site-author-name" itemprop="name">Javen</p>
              <p class="site-description motion-element" itemprop="description">柔而不弱，强而不悍<br>阳光下像个孩子<br>风雨里像个大人</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">57</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">30</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">34</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          
          
          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/Zosien" title="GitHub &rarr; https://github.com/Zosien" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="mailto:javen@duck.com" title="E-Mail &rarr; mailto:javen@duck.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
                </span>
              
            </div>
          

          
             <div class="cc-license motion-element" itemprop="license">
              
              
              
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
             </div>
          

          
          

          

          
            
          
          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#聊天机器人技术报告"><span class="nav-number">1.</span> <span class="nav-text">聊天机器人技术报告</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#编码器"><span class="nav-number">1.1.</span> <span class="nav-text">编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解码器"><span class="nav-number">1.2.</span> <span class="nav-text">解码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Teacher-Forcing"><span class="nav-number">1.3.</span> <span class="nav-text">Teacher Forcing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention"><span class="nav-number">1.4.</span> <span class="nav-text">attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关键代码分析："><span class="nav-number">1.5.</span> <span class="nav-text">关键代码分析：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果"><span class="nav-number">1.6.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考资料"><span class="nav-number">1.7.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright"><font color="white" face="STLiti">Copyright &nbsp;</font><font color="white">&copy;</font> <font color="white" face="STLiti">2018 - </font><font color="white" face="STLiti"><span itemprop="copyrightYear">2022</span></font>
  <span class="with-love" id="animate">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder" style="font-family:STLiti;color:white;">Javen . All Rights Reserved.</span>

  

  
</div>


  









        
<div class="busuanzi-count">
  <script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <font color="white" face="STLiti"><span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次
    </span><span class="post-meta-divider">|</span></font>
    <font color="white" face="STLiti">
    <span title="总字数"><i class="fa fa-edit"></i>&ensp;<span class="post-count">106.3k</span>字,</span>
    <span id="timeDate" title="网站运行时间">载入天数...</span><span id="times" title="网站运行时间">载入时分秒...</span><span class="post-meta-divider">|</span>
    </font>
  

  
    <font color="white" face="STLiti"><span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次
    </span></font>
  
</div>











        <!-- <div style="font-family:STLiti;display:inline-block;height:20px;line-height:20px;">
        <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=43030402000254" ><img src="/images/gov.png" style="float:left;"/>湘公网安备 43030402000254号</a>
        <span class="post-meta-divider" style="color: #555;">|</span><span><a href="http://www.miitbeian.gov.cn" target="_blank">湘ICP备18020535号</a></sapn>
        </div> -->
        
        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    

    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





  



  


















  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.7.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.7.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.7.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.7.0"></script>


  

  

  

  

  
<!--console.log-->

<!--此处为建站时间 -->
<script>
  var now = new Date(); 
  function createtime() { 
      var grt= new Date("05/28/2019 20:01:01");
      now.setTime(now.getTime()+250); 
      days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
      hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
      if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
      mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
      seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
      snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
      document.getElementById("timeDate").innerHTML =dnum+"&thinsp;天"; 
      document.getElementById("times").innerHTML = hnum + "&thinsp;时" + mnum + "&thinsp;分" + snum + "&thinsp;秒"; 
  } 
setInterval("createtime()",250);
</script>

<!--知乎卡片链接-->
<script type="text/javascript" src="/js/src/linkcard.js"></script>

<!--夜间模式-->
<script type="text/javascript" src="/js/src/night.js"></script>
<div class="cover"></div>

<!--崩溃欺骗-->
<!-- <script type="text/javascript" src="/js/src/crash_cheat.js"></script> -->

<!--input特效-->
<script src="/js/src/activate-power-mode.js"></script>
<script>
POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = false; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script>

<!-- 鼠标点击特效 -->

 <script type="text/javascript" src="/js/src/love.js"></script>



  
  
  

  

  
  
  

  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: true,
        appId: 'nBqcJNgT4JDUb4G29LREPoiY-gzGzoHsz',
        appKey: '3GRto2sfPhCf8DQnPl68r3RI',
        placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!填写邮箱能很快收到我的回复喔！',
        avatar:'monsterid',
        meta:guest,
        pageSize:'15' || 10,
        visitor: true
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  

  


  

  
  

  
  
  
  <script src="/lib/pangu/dist/browser/pangu.min.js"></script>
  <script type="text/javascript">pangu.spacingPage();</script>


  

  

  
  
  
  <script src="/lib/bookmark/bookmark.min.js"></script>
  <script type="text/javascript">
  
    bookmark.scrollToMark('manual', "#更多");
  
  </script>


  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      
        background-color: #1b1b1b;
        color: #b0b0b0;
        border: 1px solid #b0b0b0;
        border-radius: 3px;
      
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 9px;
      top: 4px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1;
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('复制成功')
          else $(this).text('复制失败')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
